---
title: "STA 578: Homework 2"
author: "Garrett Allen"
format: pdf
editor: visual
---

```{r}
#| label: read-data
library(tidyverse)
library(optmatch)
data <- read.csv("nhanesi_class_dataset.csv")
```

#1

```{r}
#| label: mean-imputation
data <- data %>% 
  mutate(missing_ipr = is.na(income.poverty.ratio),
         missing_diet_adeq = is.na(dietary.adequacy)) %>% 
  mutate(income.poverty.ratio = if_else(missing_ipr, 
                                        mean(income.poverty.ratio, 
                                             na.rm = TRUE),
                                        income.poverty.ratio
                                        ),
         dietary.adequacy = if_else(missing_diet_adeq, 
                                    mean(dietary.adequacy,
                                         na.rm = TRUE),
                                    dietary.adequacy)
         )
```

#2

```{r}
#| label: fit model and calculate scores
propensity_model <- glm(physically.inactive ~ 
                          age.at.interview 
                        + race 
                        + education 
                        + working.last.three.months
                        + married
                        + alcohol.consumption
                        + dietary.adequacy
                        + missing_ipr  
                        + missing_diet_adeq, 
                        family = "binomial",
                        data = data)

propensity_scores <- propensity_model %>%  
  predict(type = "response")

data <- data %>% 
  mutate(propensity_score = propensity_scores)

```

```{r}
#| label: filter units
max_min_tbl <- data %>%
  group_by(physically.inactive) %>% 
  summarize(max_prop = max(propensity_score),
            min_prop = min(propensity_score))

max_min_tbl

max_control = max_min_tbl$max_prop[1]
min_treated = max_min_tbl$min_prop[2]

filtered_data <- data %>% 
  filter(propensity_score >= min_treated & propensity_score <= max_control)

num_excluded = nrow(data) - nrow(filtered_data)

tibble(`Number of Units Excluded` = num_excluded)
```

#3

```{r}
#| label: opt-match-code
library(caret)
library(ggplot2)  
library(optmatch)
library(tableone)
# This R code is a slight modification of code in Prof. Dylan Small's lecture
# It is used to construct rank based Mahalanobis distance with propensity score caliper


optmatch_caliper<-function(datatemp,nocontrols.per.match,ps.formula,mahal.formula,calipersd=.5){
  # Comment about this code and subsequent matching code
  # There is assumed to be no missing data in the variables that go into the 
  # propensity score model so that there is a propensity score for every variable in 
  # the data frame. 
  # Fit a propensity score using logistic regression with each covariate entering 
  # linearly into the logistic link function
  # Put x=TRUE in order to have model object include design matrix
  propscore.model=glm(ps.formula,family=binomial,data=datatemp)
  
  # This model is to obtain model.matrix for mahalanobis distance.
  mahal.model=glm(mahal.formula,family=binomial,x=TRUE,y=TRUE,data=datatemp) 
  
  datatemp$treated=mahal.model$y
  datatemp$treatment=datatemp$treated
  # Use the caret package to include all categories of categorical variables (i.e.,
  # do not leave out one category) in X matrix
  dmy=dummyVars(mahal.model$formula,data=datatemp)
  Xmat=data.frame(predict(dmy,newdata=datatemp))
  # Matrix of covariates to include in the Mahalanobis distance, for now include all 
  # covariates
  Xmatmahal=Xmat
  
  treated=datatemp$treated
  datatemp$logit.ps=predict(propscore.model) 

  
  # Use Hansen (2009)â€™s rule for removing subjects who lack overlap 
  logit.propscore=datatemp$logit.ps
  pooled.sd.logit.propscore=sqrt(var(logit.propscore[datatemp$treatment==1])/2+var(logit.propscore[datatemp$treatment==0])/2)
  min.treated.logit.propscore=min(logit.propscore[datatemp$treatment==1])
  max.control.logit.propscore=max(logit.propscore[datatemp$treatment==0])
  # How many treated and control subjects lack overlap by Hansen's criterion
  no.treated.lack.overlap=sum(logit.propscore[datatemp$treatment==1]>(max.control.logit.propscore+.5*pooled.sd.logit.propscore))
  no.control.lack.overlap=sum(logit.propscore[datatemp$treatment==0]<(min.treated.logit.propscore-.5*pooled.sd.logit.propscore))
  # If there are subjects who lack overlap, remove them from the datatemp dataset
  datatemp.original=datatemp
  datatemp.full=datatemp
  Xmat.original=Xmat
  Xmat.full=Xmat
  if(no.treated.lack.overlap+no.control.lack.overlap>0){
    which.remove=which((logit.propscore>(max.control.logit.propscore+.5*pooled.sd.logit.propscore))|(logit.propscore<(min.treated.logit.propscore-.5*pooled.sd.logit.propscore)))
    datatemp=datatemp[-which.remove,]
    datatemp.full=rbind(datatemp,datatemp.original[which.remove,])
    Xmat=Xmat[-which.remove,]
    Xmat.full=rbind(Xmat,Xmat.original[which.remove,])
    Xmatmahal=Xmatmahal[-which.remove,]
  }
  # For the purposes of balance checking later, in datatemp.full, append 
  # the removed rows of datatemp to the end of datatemp
  
  # Make the rownames in datatemp be 1:number of rows
  rownames(datatemp)=seq(1,nrow(datatemp),1) 
  
  # Function for computing 
  # rank based Mahalanobis distance.  Prevents an outlier from
  # inflating the variance for a variable, thereby decreasing its importance.
  # Also, the variances are not permitted to decrease as ties 
  # become more common, so that, for example, it is not more important
  # to match on a rare binary variable than on a common binary variable
  # z is a vector, length(z)=n, with z=1 for treated, z=0 for control
  # X is a matrix with n rows containing variables in the distance
  
  smahal=
    function(z,X){
      X<-as.matrix(X)
      n<-dim(X)[1]
      rownames(X)<-1:n
      k<-dim(X)[2]
      m<-sum(z)
      for (j in 1:k) X[,j]<-rank(X[,j])
      cv<-cov(X)
      vuntied<-var(1:n)
      rat<-sqrt(vuntied/diag(cv))
      cv<-diag(rat)%*%cv%*%diag(rat)
      out<-matrix(NA,m,n-m)
      Xc<-X[z==0,]
      Xt<-X[z==1,]
      rownames(out)<-rownames(X)[z==1]
      colnames(out)<-rownames(X)[z==0]
      library(MASS)
      icov<-ginv(cv)
      for (i in 1:m) out[i,]<-mahalanobis(Xc,Xt[i,],icov,inverted=T)
      out
    }
  
  # Function for adding a propensity score caliper to a distance matrix dmat
  # calipersd is the caliper in terms of standard deviation of the logit propensity scoe
  addcaliper=function(dmat,z,logitp,calipersd=.5,penalty=1000){
    # Pooled within group standard devation
    sd.logitp=sqrt((sd(logitp[z==1])^2+sd(logitp[z==0])^2)/2)
    adif=abs(outer(logitp[z==1],logitp[z==0],"-"))
    adif=(adif-(calipersd*sd.logitp))*(adif>(calipersd*sd.logitp))
    dmat=dmat+adif*penalty
    dmat
  }
  
  
  # Rank based Mahalanobis distance
  distmat=smahal(datatemp$treated,Xmatmahal)
  # Add caliper
  distmat=addcaliper(distmat,datatemp$treated,datatemp$logit.ps,calipersd=.5)
  
  
  # Label the rows and columns of the distance matrix by the rownames in datatemp
  rownames(distmat)=rownames(datatemp)[datatemp$treated==1]
  colnames(distmat)=rownames(datatemp)[datatemp$treated==0]
  
  
  matchvec=pairmatch(distmat,controls=nocontrols.per.match,data=datatemp)
  datatemp$matchvec=matchvec
  m <- matchvec
  ## Create a matrix saying which control units each treated unit is matched to
  ## Create vectors of the subject indices of the treatment units ordered by
  ## their matched set and corresponding control unit
  treated.subject.index=rep(0,sum(treated==1))
  matched.control.subject.index.mat=matrix(rep(0,nocontrols.per.match*length(treated.subject.index)),ncol=nocontrols.per.match)
  matchedset.index=substr(matchvec,start=3,stop=10)
  matchedset.index.numeric=as.numeric(matchedset.index)
  
  for(i in 1:length(treated.subject.index)){
    matched.set.temp=which(matchedset.index.numeric==i)
    treated.temp.index=which(datatemp$treated[matched.set.temp]==1)
    treated.subject.index[i]=matched.set.temp[treated.temp.index]
    matched.control.subject.index.mat[i,]=matched.set.temp[-treated.temp.index]
  }
  matched.control.subject.index=matched.control.subject.index.mat
  
  Xmat.without.missing<-Xmat.full
  treatedmat=Xmat.without.missing[datatemp.full$treated==1,];
  # Standardized differences before matching
  controlmat.before=Xmat.without.missing[datatemp.full$treated==0,];
  controlmean.before=apply(controlmat.before,2,mean,na.rm=TRUE);
  
  treatmean=apply(treatedmat,2,mean,na.rm=TRUE);
  treatvar=apply(treatedmat,2,var,na.rm=TRUE);
  controlvar=apply(controlmat.before,2,var,na.rm=TRUE);
  stand.diff.before=(treatmean-controlmean.before)/sqrt((treatvar+controlvar)/2);
  
  treatmat.after=Xmat.without.missing[treated.subject.index,]
  controlmat.after=Xmat.without.missing[matched.control.subject.index,];
  controlmean.after=apply(controlmat.after,2,mean,na.rm=TRUE);
  treatmean.after=apply(treatmat.after,2,mean,na.rm=TRUE)
  stand.diff.after=(treatmean-controlmean.after)/sqrt((treatvar+controlvar)/2)
  
  res.stand.diff<-cbind(stand.diff.before,stand.diff.after)
  res.mean<-cbind(treatmean.after,controlmean.before,controlmean.after)
  print(round(res.stand.diff,2))
  print(round(res.mean,2))
  
  abs.stand.diff.before=stand.diff.before[-1]
  abs.stand.diff.after=stand.diff.after[-1]
  covariates=names(stand.diff.before[-1])
  plot.dataframe=data.frame(abs.stand.diff=c(abs.stand.diff.before,abs.stand.diff.after),covariates=rep(covariates,2),type=c(rep("Before",length(covariates)),rep("After",length(covariates))))
  p<-ggplot(plot.dataframe,aes(x=abs.stand.diff,y=covariates))+geom_point(size=2,aes(shape=type))+scale_shape_manual(values=c(4,1))+geom_vline(xintercept=c(-.1,.1),lty=2)+xlab("standardized differences in means")+ ylab("")
  return(list(p=p,datatemp=datatemp,treated.subject.index=treated.subject.index,
              matched.control.subject.index=matched.control.subject.index,
              res.stand.diff=res.stand.diff,res.mean=res.mean))
}


```

```{r}
#| label: matching
ps.formula=physically.inactive~
  sex+
  smoking.status+
  income.poverty.ratio+
  age.at.interview+
  race+
  education+
  working.last.three.months+
  married+
  alcohol.consumption+
  dietary.adequacy+
  missing_ipr+
  missing_diet_adeq

mahal.formula=physically.inactive~sex+smoking.status+age.at.interview

match_res1 <- optmatch_caliper(filtered_data,
                             nocontrols.per.match = 1, 
                             calipersd=0.5,
                             ps.formula=ps.formula,
                             mahal.formula=mahal.formula)

df_treated<-match_res1$datatemp[match_res1$treated.subject.index,]
df_control<-match_res1$datatemp[match_res1$matched.control.subject.index,]
df_matched<-rbind(df_treated,df_control)


```

```{r}
#| label: print-tbl-one
# Table One

library(kableExtra)
tbl1 <- CreateTableOne(vars=names(filtered_data)[-c(1:2)],strata="physically.inactive",data=df_matched)

print(tbl1,showAllLevels = TRUE,smd=FALSE,quote=FALSE, noSpaces = TRUE, printToggle = TRUE) %>% 
  as.data.frame() %>% 
  filter(p != "") %>% 
  rownames_to_column() %>% 
  filter(rowname %in% c("sex....", "age.at.interview..mean..SD..","smoking.status....")) %>% 
  mutate(rowname = c("sex","smoking","age_at_interview")) %>% 
  dplyr::select(rowname, level, "FALSE", "TRUE", p)
```

```{r}
#| label: love-plot

# Love plot
match_res1$p +theme_bw() + labs(x = "Standardized Differences in Means",
                                shape = "Type",
                                title = "Balance appears to be good, differences in acceptable range")
```

```{r}
#| label: standardized_differences
standardized_differences <- match_res1$res.stand.diff %>% 
  data.frame()

standardized_differences
```

Balance appears good, as all the standardized differences are very small after matching relative to their standardized differences before, indicating that we have matched well on each of our covariates.

#4

```{r}
#| label: matching-two

match_res2 <- optmatch_caliper(filtered_data,
                             nocontrols.per.match = 2, 
                             calipersd=0.5,
                             ps.formula=ps.formula,
                             mahal.formula=mahal.formula)
```

```{r}
#| label: std_differences-two
standardized_differences_2 <- match_res2$res.stand.diff %>% 
  data.frame()

standardized_differences_2
```

```{r}
#| label: love-plot-two

# Love plot
match_res2$p +theme_bw() + labs(x = "Standardized Differences in Means",
                                shape = "Type",
                                title = "Balance appears to be good, differences in acceptable range")
```
```{r}
#| label: print-tbl-two
df_treated_2<-match_res2$datatemp[match_res2$treated.subject.index,]
df_control_2<-match_res2$datatemp[match_res2$matched.control.subject.index,]
df_matched_2<-rbind(df_treated_2,df_control_2)

tbl2 <- CreateTableOne(vars=names(filtered_data)[-c(1:2)],strata="physically.inactive",data=df_matched_2)

print(tbl2,showAllLevels = TRUE,smd=FALSE,quote=FALSE, noSpaces = TRUE, printToggle = TRUE) %>% 
  as.data.frame() %>% 
  filter(p != "") %>% 
  rownames_to_column() %>% 
  filter(rowname %in% c("sex....", "age.at.interview..mean..SD..","smoking.status....")) %>% 
  mutate(rowname = c("sex","smoking","age_at_interview")) %>% 
  dplyr::select(rowname, level, "FALSE", "TRUE", p)

```
Balance still appears to be good, all of the standardized differences are within an acceptable range (less than .1 in magnitude), but they are worse than in the single matching case. Let's try matching on 3 controls to see how well that does.

```{r}
#| label: matching-two

match_res3 <- optmatch_caliper(filtered_data,
                             nocontrols.per.match = 3, 
                             calipersd=0.5,
                             ps.formula=ps.formula,
                             mahal.formula=mahal.formula)
```

```{r}
#| label: std_differences-three
standardized_differences_3 <- match_res3$res.stand.diff %>% 
  data.frame()

standardized_differences_3
```

```{r}
#| label: love-plot-three

# Love plot
match_res3$p +theme_bw() + labs(x = "Standardized Differences in Means",
                                shape = "Type",
                                title = "Balance appears to be good, differences in acceptable range")
```

```{r}
#| label: print-tbl-three
df_treated_3<-match_res3$datatemp[match_res3$treated.subject.index,]
df_control_3<-match_res3$datatemp[match_res3$matched.control.subject.index,]
df_matched_3<-rbind(df_treated_3,df_control_3)

tbl3 <- CreateTableOne(vars=names(filtered_data)[-c(1:2)],strata="physically.inactive",data=df_matched_3)

print(tbl3,showAllLevels = TRUE,smd=FALSE,quote=FALSE, noSpaces = TRUE, printToggle = TRUE) %>% 
  as.data.frame() %>% 
  filter(p != "") %>% 
  rownames_to_column() %>% 
  filter(rowname %in% c("sex....", "age.at.interview..mean..SD..","smoking.status....")) %>% 
  mutate(rowname = c("sex","smoking","age_at_interview")) %>% 
  dplyr::select(rowname, level, "FALSE", "TRUE", p)

```

Balance has gotten bad now; not only are most of them greater in magnitude in .1, but one of them is greater in magnitude .2. There is not a good balance anymore. T-test for differences
are significant. 

#5

best matching; first one

#6

```{r}
#| label: wilcoxon_confint
library(exactRankTests)
wilcox.exact(df_treated$years.lived.since.1971.up.to.1992,
df_control$years.lived.since.1971.up.to.1992,
paired=TRUE,conf.int=TRUE,exact = TRUE)
```

```{r}
#| label: estimate
matched.reg.model=lm(years.lived.since.1971.up.to.1992~physically.inactive
                     +matchvec
+sex
+smoking.status
+income.poverty.ratio
+age.at.interview
+race
+education
+working.last.three.months
+married
+alcohol.consumption
+dietary.adequacy
+missing_ipr
+missing_diet_adeq,data=df_matched)

coefficient_tbl <- c(coef(matched.reg.model)[2], confint(matched.reg.model)[2,]) %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  pivot_wider(names_from = rowname, values_from = ".") %>% 
  rename(estimate = physically.inactiveTRUE,
         lower_int_2.5 = `2.5 %`,
         upper_int_97.5 = `97.5 %`) %>% 
  as.data.frame()

rownames(coefficient_tbl) <- "physically.inactive"

coefficient_tbl
```

later: add best one from q5

#7

interpretation q

#8

```{r}

#| label: find-gamma
diff=df_treated$years.lived.since.1971.up.to.1992-
df_control$years.lived.since.1971.up.to.1992

library(DOS)

senWilcox(diff, gamma = 1, conf.int = FALSE, alpha = 0.05, alternative = "less")
senWilcox(diff, gamma = 1.6, conf.int = FALSE, alpha = 0.05, alternative = "less")
senWilcox(diff, gamma = 1.997, conf.int = FALSE, alpha = 0.05, alternative = "less")
```

#9

```{r}
#| label: outcome-reg
library(CausalGAM)
library(ebal)
```

#10

```{r}
#| label: sensitivity-analysis

library(devtools)
install_github("qingyuanzhao/bootsens")
library(bootsens)
```

```{r}
#| label: sensitivity-analysis-2
# ipw
A<-data$physically.inactive
X<-model.matrix(glm(ps.formula,family=binomial,data=data))
X<-X[,-1]
Y<-data$years.lived.since.1971.up.to.1992
## IPW, assuming no unmeasure confounder (i.e. gamma = 0 or Gamma = e^0 = 1)
extrema.os(A, X, Y) # point estimate
bootsens.os(A, X, Y, parallel = FALSE) # bootstrap confidence interval (CI)
## IPW, Sensitivity analysis (gamma = log(1.2), i.e. Gamma = 1.2)
extrema.os(A, X, Y, gamma = log(1.2)) # point estimate
bootsens.os(A, X, Y, gamma = log(1.2), parallel = FALSE) # bootstrap CI
# the IPW estimator is robust to Gamma=1.2
## AIPW, assuming no unmeasure confounder (i.e. gamma = 0 or Gamma = e^0 = 1)
extrema.os(A, X, Y, reg.adjust = TRUE) # point estimate
bootsens.os(A, X, Y, reg.adjust = TRUE, parallel = FALSE) # bootstrap CI
## AIPW, Sensitivity analysis (Gamma = exp(gamma))
extrema.os(A, X, Y, reg.adjust = TRUE,gamma = log(1.2)) # point estimate
bootsens.os(A, X, Y, reg.adjust = TRUE, gamma = log(1.2),parallel = FALSE) # bootstrap CI
bootsens.os(A, X, Y, reg.adjust = TRUE, gamma = log(1.4),parallel = FALSE) # bootstrap CI
# AIPW is robust to Gamma=1.4, because it has higher power compared to the IPW
```
